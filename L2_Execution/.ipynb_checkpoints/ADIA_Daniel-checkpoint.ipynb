{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setup\n",
    "import sys\n",
    "\n",
    "Github_folder_path = r'../'\n",
    "if Github_folder_path not in sys.path:\n",
    "    sys.path.append(Github_folder_path)\n",
    "\n",
    "from L0_Library.config import *\n",
    "from L1_Dev.LOB_data import DataLOB\n",
    "\n",
    "# Set the display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Kd3SD95fr4db0LcTLlYoJq",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "data_lob = DataLOB(dataset=\"XNAS.ITCH\", API_key=\"db-SEbmRhQ3ekjnrdRQLfK4iDYJhVkrL\", resample_freq=\"1T\",\n",
    "                   date=datetime(2023, 9, 5, 9, 30), end_hour=10, end_minute=0, t_days=3)\n",
    "\n",
    "df, count_days = data_lob.build_data_set(\"TSLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mid_last = data_lob.get_mid_price(df_input=df, type_mid=\"last\", drop_na=False)\n",
    "df_ret_last = data_lob.calculate_return(df_input=df_mid_last, column_name=\"mid-price_last\")\n",
    "df_bv_last = data_lob.sqrt_root_average_realised_bipower_variation(df_input=df_ret_last, column_name=\"log-returns\", k=10)\n",
    "\n",
    "df_mid_mean = data_lob.get_mid_price(df_input=df, type_mid=\"mean\", drop_na=False)\n",
    "df_ret_mean = data_lob.calculate_return(df_input=df_mid_mean, column_name=\"mid-price_mean\")\n",
    "df_bv_mean = data_lob.sqrt_root_average_realised_bipower_variation(df_input=df_ret_mean, column_name=\"log-returns\", k=10)\n",
    "\n",
    "df_mid_vwmp = data_lob.get_mid_price(df_input=df, type_mid=\"vwmp\", drop_na=False)\n",
    "df_ret_vwmp = data_lob.calculate_return(df_input=df_mid_vwmp, column_name=\"mid-price_vwmp\")\n",
    "df_bv_vwmp = data_lob.sqrt_root_average_realised_bipower_variation(df_input=df_ret_vwmp, column_name=\"log-returns\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrames side by side\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display(HTML(html_str.replace('table','table style=\"display:inline\"')))\n",
    "\n",
    "display_side_by_side(df_bv_last, df_bv_vwmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "o0dmAZX6wWvQNMhdduscns",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# PIPELINE:\n",
    "# Get news data characterised by its title, timestamp, and a list of tickers\n",
    "# Calculate the mid-price time series DONE\n",
    "# Calculate the 1-minute return time series DONE\n",
    "# Calculate the local volatility over a rolling time window of length K = 390\n",
    "# Compute the “jump-scores”\n",
    "# Use the Gumbel distribution and the \"Extreme Value Theory\" to identify jumps\n",
    "## Change from analyzing single jumps to clusters of them\n",
    "# Use the inter-time distribution to compare with Bernouilli null-hypothesis on jumps occurring with probability p to cluster jumps\n",
    "# Use the same technique to cluster news\n",
    "# Mark as news related (or exogenous) those clusters of jumps which started up to one minute before and up to four minutes after the beginning of a cluster of news.\n",
    "# Remove market-wide or sector-wide news (affecting more than 10% - 30 stocks)\n",
    "# Remove any cluster of jumps happening within 100 minutes to account for contamination effect\n",
    "# Claculate Kendall's correlation to characterize the structure of the internal structure of clusters\n",
    "# Characterize EMC and SEC jumps profiles using the following 5 measures: 1. Instantaneous jump-score. 2. Exponential moving average of past excess volatilty. 3.  Normalized past price trend. 4. Binarized past price trend. 5. Instantaneous average LOB sparsity.\n",
    "\n",
    "\n",
    "\n",
    "# Implement: w only keep trading days with at least 300 recorded price changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ReS9KyUbx2vUTWYKMHHFTM",
     "type": "MD"
    }
   },
   "source": [
    "Sample: consists of $T$ days of $M$ equally spaced and continuously compounded intraday return observations $r_i(i=1, \\ldots, M T)$ of a financial asset. We normalize the length of one trading day to unity such that $\\Delta=1 / M$ equals the time elapsed between two consecutive return observations. Hence, $r_i$ equals the return over the time interval $[(i-1) \\Delta, i \\Delta]$.\n",
    "\n",
    "The local windows: obtained by a division of $[0, T]$ in time intervals of length $\\lambda$. As such, the $MT$ observations are divided in groups of $\\lfloor\\lambda / \\Delta\\rfloor$ contiguous observations (i.e. $\\lfloor 3.7\\rfloor=3$).\n",
    "\n",
    "$N_i$: collection of indices $j$ that belong to the same window as $i$.\n",
    "\n",
    "Standardization condition: the squared periodicity factor has mean one over the local window thus implies that for all $i=1, \\ldots, TM$\n",
    "$$\n",
    "\\frac{1}{\\lfloor\\lambda / \\Delta\\rfloor} \\sum_{j \\in N_i} f_j^2=1 .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The next question addresses how to disentangle jumps from the continuous component. For this purpose, measures of diffusive volatility are necessary. Barndorff-Nielsen and Shephard (2004) tackle this issue by proposing the bipower variation (BPV) measure, which is computed as the scaled summation of the product of adjacent absolute returns. The BPV is a consistent estimator of integrated volatility, and allows to decompose the realized volatility into its diffusive and non-diffusive parts. As the sampling frequency increases, the presence of jumps should have no impact, because the return representing the jump is multiplied by a non-jump return which tends to zero asymptotically. This is true in case of rare jumps (one each day), when the probability of two consecutive jumps is negligible.\n",
    "\n",
    "The estimator of Taylor and Xu (1997) is basically the sd across i of a given time of a given day over sd or the local window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "ADIA",
   "language": "python",
   "name": "adia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
